{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Test the TFF is working:\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fixed vocabularly of ASCII chars that occur in the works of Shakespeare and Dickens:\n",
    "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(batch_size):\n",
    "  urls = {\n",
    "      1: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel',\n",
    "      8: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel'}\n",
    "  assert batch_size in urls, 'batch_size must be in ' + str(urls.keys())\n",
    "  url = urls[batch_size]\n",
    "  local_file = tf.keras.utils.get_file(os.path.basename(url), origin=url)  \n",
    "  return tf.keras.models.load_model(local_file, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # From https://www.tensorflow.org/tutorials/sequences/text_generation\n",
    "  num_generate = 200\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  text_generated = []\n",
    "  temperature = 1.0\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(\n",
    "        predictions, num_samples=1)[-1, 0].numpy()\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel\n",
      "16195584/16193984 [==============================] - 22s 1us/step\n",
      "What of TensorFlow Federated, you ask? The rather\n",
      "forefinger at the Aristocrat who had been spaced in a\n",
      "florid give me a child.\"\n",
      "\n",
      "\"Yes, sir. Nothing exceedingly needing and wept for me! Wy would have falles\n",
      "from the ground by a red ca\n"
     ]
    }
   ],
   "source": [
    "# Text generation requires a batch_size=1 model.\n",
    "keras_model_batch1 = load_model(batch_size=1)\n",
    "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess federated Shakespeare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tff-datasets-public/shakespeare.tar.bz2\n",
      "1851392/1848122 [==============================] - 17s 9us/step\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Live regist'red upon our brazen tombs,\\nAnd then grace us in the disgrace of death;\\nWhen, spite of cormorant devouring Time,\\nTh' endeavour of this present breath may buy\\nThat honour which shall bate his scythe's keen edge,\\nAnd make us heirs of all eternity.\\nTherefore, brave conquerors- for so you are\\nThat war against your own affections\\nAnd the huge army of the world's desires-\\nOur late edict shall strongly stand in force:\\nNavarre shall be the wonder of the world;\\nOur court shall be a little Academe,\\nStill and contemplative in living art.\\nYou three, Berowne, Dumain, and Longaville,\\nHave sworn for three years' term to live with me\\nMy fellow-scholars, and to keep those statutes\\nThat are recorded in this schedule here.\\nYour oaths are pass'd; and now subscribe your names,\\nThat his own hand may strike his honour down\\nThat violates the smallest branch herein.\\nIf you are arm'd to do as sworn to do,\\nSubscribe to your deep oaths, and keep it too.\\nYour oath is pass'd to pass away from these.\", shape=(), dtype=string)\n",
      "tf.Tensor(b'this?\\nDid you hear the proclamation?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Here the play is \"The Tragedy of King Lear\" and the character is \"King\".\n",
    "raw_example_dataset = train_data.create_tf_dataset_for_client(\n",
    "    'THE_TRAGEDY_OF_KING_LEAR_KING')\n",
    "# To allow for future extensions, each entry x\n",
    "# is an OrderedDict with a single key 'snippets' which contains the text.\n",
    "for x in raw_example_dataset.take(2):\n",
    "  print(x['snippets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pre-processing parameters\n",
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 100  # For dataset shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a lookup table to map string chars to indexes,\n",
    "# using the vocab loaded above:\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=vocab, values=tf.constant(list(range(len(vocab))),\n",
    "                                       dtype=tf.int64)),\n",
    "    default_value=0)\n",
    "\n",
    "\n",
    "def to_ids(x):\n",
    "  s = tf.reshape(x['snippets'], shape=[1])\n",
    "  chars = tf.strings.bytes_split(s).values\n",
    "  ids = table.lookup(chars)\n",
    "  return ids\n",
    "\n",
    "\n",
    "def split_input_target(chunk):\n",
    "  input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
    "  target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
    "  return (input_text, target_text)\n",
    "\n",
    "\n",
    "def preprocess(dataset):\n",
    "  return (\n",
    "      # Map ASCII chars to int64 indexes using the vocab\n",
    "      dataset.map(to_ids)\n",
    "      # Split into individual chars\n",
    "      .unbatch()\n",
    "      # Form example sequences of SEQ_LENGTH +1\n",
    "      .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "      # Shuffle and form minibatches\n",
    "      .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "      # And finally split into (input, target) tuples,\n",
    "      # each of length SEQ_LENGTH.\n",
    "      .map(split_input_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(8, 100), dtype=tf.int64, name=None), TensorSpec(shape=(8, 100), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "example_dataset = preprocess(raw_example_dataset)\n",
    "print(example_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "\n",
    "  def __init__(self, name='accuracy', dtype=tf.float32):\n",
    "    super().__init__(name, dtype=dtype)\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    y_true = tf.reshape(y_true, [-1, 1])\n",
    "    y_pred = tf.reshape(y_pred, [-1, len(vocab), 1])\n",
    "    return super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel\n",
      "16195584/16193984 [==============================] - 35s 2us/step\n",
      "Evaluating on an example Shakespeare character: 0.400500\n",
      "Expected accuracy for random guessing: 0.012\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on completely random data: 0.015\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8  # The training and eval batch size for the rest of this tutorial.\n",
    "keras_model = load_model(batch_size=BATCH_SIZE)\n",
    "keras_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[FlattenedCategoricalAccuracy()])\n",
    "\n",
    "# Confirm that loss is much lower on Shakespeare than on random data\n",
    "loss, accuracy = keras_model.evaluate(example_dataset.take(5), verbose=0)\n",
    "print(\n",
    "    'Evaluating on an example Shakespeare character: {a:3f}'.format(a=accuracy))\n",
    "\n",
    "# As a sanity check, we can construct some completely random data, where we expect\n",
    "# the accuracy to be essentially random:\n",
    "random_guessed_accuracy = 1.0 / len(vocab)\n",
    "print('Expected accuracy for random guessing: {a:.3f}'.format(\n",
    "    a=random_guessed_accuracy))\n",
    "random_indexes = np.random.randint(\n",
    "    low=0, high=len(vocab), size=1 * BATCH_SIZE * (SEQ_LENGTH + 1))\n",
    "data = collections.OrderedDict(\n",
    "    snippets=tf.constant(\n",
    "        ''.join(np.array(vocab)[random_indexes]), shape=[1, 1]))\n",
    "random_dataset = preprocess(tf.data.Dataset.from_tensor_slices(data))\n",
    "loss, accuracy = keras_model.evaluate(random_dataset, steps=10, verbose=0)\n",
    "print('Evaluating on completely random data: {a:.3f}'.format(a=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model on FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the keras_model inside `create_tff_model()`, which TFF will\n",
    "# call to produce a new copy of the model inside the graph that it will \n",
    "# serialize. Note: we want to construct all the necessary objects we'll need \n",
    "# _inside_ this method.\n",
    "def create_tff_model():\n",
    "  # TFF uses an `input_spec` so it knows the types and shapes\n",
    "  # that your model expects.\n",
    "  input_spec = example_dataset.element_spec\n",
    "  keras_model_clone = tf.keras.models.clone_model(keras_model)\n",
    "  return tff.learning.from_keras_model(\n",
    "      keras_model_clone,\n",
    "      input_spec=input_spec,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[FlattenedCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Leyan\\anaconda3\\envs\\audi9\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Leyan\\anaconda3\\envs\\audi9\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['gru_1/gru_cell/kernel:0', 'gru_1/gru_cell/recurrent_kernel:0', 'gru_1/gru_cell/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['gru_1/gru_cell/kernel:0', 'gru_1/gru_cell/recurrent_kernel:0', 'gru_1/gru_cell/bias:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "# This command builds all the TensorFlow graphs and serializes them: \n",
    "fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.402, accuracy=0.134\n"
     ]
    }
   ],
   "source": [
    "state = fed_avg.initialize()\n",
    "state, metrics = fed_avg.next(state, [example_dataset.take(5)])\n",
    "train_metrics = metrics['train']\n",
    "print('loss={l:.3f}, accuracy={a:.3f}'.format(\n",
    "    l=train_metrics['loss'], a=train_metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we train on the same three clients each round, only considering two minibatches for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000200C4C009D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x00000200C4C009D0>. It was defined on this line: [data(client, test_data) for client in clients], which must contain a single lambda with matching signature. To avoid ambiguity, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000200C4C009D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x00000200C4C009D0>. It was defined on this line: [data(client, test_data) for client in clients], which must contain a single lambda with matching signature. To avoid ambiguity, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000200C4C009D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x00000200C4C009D0>. It was defined on this line: [data(client, test_data) for client in clients], which must contain a single lambda with matching signature. To avoid ambiguity, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def data(client, source=train_data):\n",
    "  return preprocess(source.create_tf_dataset_for_client(client)).take(5)\n",
    "\n",
    "\n",
    "clients = [\n",
    "    'ALL_S_WELL_THAT_ENDS_WELL_CELIA', 'MUCH_ADO_ABOUT_NOTHING_OTHELLO',\n",
    "]\n",
    "\n",
    "train_datasets = [data(client) for client in clients]\n",
    "\n",
    "# We concatenate the test datasets for evaluation with Keras by creating a \n",
    "# Dataset of Datasets, and then identity flat mapping across all the examples.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    [data(client, test_data) for client in clients]).flat_map(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "\tEval: loss=3.068, accuracy=0.416\n",
      "\tTrain: loss=4.341, accuracy=0.197\n",
      "Round 1\n",
      "\tEval: loss=4.393, accuracy=0.027\n",
      "\tTrain: loss=4.248, accuracy=0.169\n",
      "Round 2\n",
      "\tEval: loss=4.292, accuracy=0.163\n",
      "\tTrain: loss=4.100, accuracy=0.227\n",
      "Round 3\n",
      "\tEval: loss=4.103, accuracy=0.193\n",
      "\tTrain: loss=3.994, accuracy=0.208\n",
      "Round 4\n",
      "\tEval: loss=4.031, accuracy=0.182\n",
      "\tTrain: loss=3.893, accuracy=0.215\n",
      "Final evaluation\n",
      "\tEval: loss=3.952, accuracy=0.171\n"
     ]
    }
   ],
   "source": [
    "NUM_ROUNDS = 5\n",
    "\n",
    "# The state of the FL server, containing the model and optimization state.\n",
    "state = fed_avg.initialize()\n",
    "\n",
    "# Load our pre-trained Keras model weights into the global model state.\n",
    "state = tff.learning.state_with_new_model_weights(\n",
    "    state,\n",
    "    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],\n",
    "    non_trainable_weights=[\n",
    "        v.numpy() for v in keras_model.non_trainable_weights\n",
    "    ])\n",
    "\n",
    "\n",
    "def keras_evaluate(state, round_num):\n",
    "  # Take our global model weights and push them back into a Keras model to\n",
    "  # use its standard `.evaluate()` method.\n",
    "  keras_model = load_model(batch_size=BATCH_SIZE)\n",
    "  keras_model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[FlattenedCategoricalAccuracy()])\n",
    "  state.model.assign_weights_to(keras_model)\n",
    "  loss, accuracy = keras_model.evaluate(example_dataset, steps=2, verbose=0)\n",
    "  print('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))\n",
    "\n",
    "\n",
    "for round_num in range(NUM_ROUNDS):\n",
    "  print('Round {r}'.format(r=round_num))\n",
    "  keras_evaluate(state, round_num)\n",
    "  state, metrics = fed_avg.next(state, train_datasets)\n",
    "  train_metrics = metrics['train']\n",
    "  print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(\n",
    "      l=train_metrics['loss'], a=train_metrics['accuracy']))\n",
    "\n",
    "print('Final evaluation')\n",
    "keras_evaluate(state, NUM_ROUNDS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What of TensorFlow Federated, you ask? Sy.\r\n",
      "\r\n",
      "\"Go Ind feel now.\"\r\n",
      "\r\n",
      "\"Not of Beauvais.\"\r\n",
      "\r\n",
      "\"I see that there mooner and few, at least, so expect to death; but, I had\r\n",
      "happened to composition that I do this young lady--and no man had additio\n"
     ]
    }
   ],
   "source": [
    "# Set our newly trained weights back in the originally created model.\n",
    "keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
    "# Text generation requires batch_size=1\n",
    "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audi9",
   "language": "python",
   "name": "audi9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
